```
Implement an autoregressive encoder transformer (mini gpt) in c# as concisely and completely as possible without an external library.  Include full implementations for all relevant components such as

   - multi-layer attention
   - multi-head self-attention
   - rotational positional encoding
   - layer normalization
   - backpropagation
   - backward pass computations
   - parameter update logic in the optimizer

Always use matrices and vectors instead of tensors.  Whenever possible, break up matrix operations into vector operations.

Include code for training and next token prediction.  Compute loss and perplexity at each training step.  No batching is needed.

Output all the code, and do not omit any details.
```


```
Implement an autoregressive encoder transformer (mini gpt) in c# as concisely and completely as possible without an external library.  Include full implementations for all relevant components such as

   - multi-layer attention
   - multi-head causal self-attention
   - regularization
   - residual dropout
   - causal mask
   - backpropagation and backward pass computations
   - parameter update logic in the optimizer

Always use matrices and vectors instead of tensors.  Whenever possible, break up matrix operations into vector operations.

Include code for training and next token prediction.

Output all the code, and do not omit any details.
```